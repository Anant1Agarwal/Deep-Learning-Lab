{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WEEK 2","metadata":{}},{"cell_type":"markdown","source":"## Sample Questions","metadata":{}},{"cell_type":"markdown","source":"#### Q1. set up simple graph relating x,y and z","metadata":{"execution":{"iopub.status.busy":"2025-01-10T13:56:36.903573Z","iopub.execute_input":"2025-01-10T13:56:36.904051Z","iopub.status.idle":"2025-01-10T13:56:36.908589Z","shell.execute_reply.started":"2025-01-10T13:56:36.904016Z","shell.execute_reply":"2025-01-10T13:56:36.907440Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nx=torch.tensor(3.5,requires_grad=True)\n\ny=x*x\n\nz=2*y+3\n\nprint(\"x: \",x)\nprint(\"\\ny=x*x: \",y)\nprint(\"\\nz=2*y+3: \",z)\n\n# work out gradients\nz.backward()\n\nprint(\"\\nWorking out gradients dz/dx\")\n\n# what is gradient at x=3.5 ?\nprint(\"Gradient at x=3.5: \",x.grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:09:37.007755Z","iopub.execute_input":"2025-01-10T14:09:37.008197Z","iopub.status.idle":"2025-01-10T14:09:37.021234Z","shell.execute_reply.started":"2025-01-10T14:09:37.008155Z","shell.execute_reply":"2025-01-10T14:09:37.020054Z"}},"outputs":[{"name":"stdout","text":"x:  tensor(3.5000, requires_grad=True)\n\ny=x*x:  tensor(12.2500, grad_fn=<MulBackward0>)\n\nz=2*y+3:  tensor(27.5000, grad_fn=<AddBackward0>)\n\nWorking out gradients dz/dx\nGradient at x=3.5:  tensor(14.)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"#### Q2 consider the function f(x)=(x-2)^2. compute d(f(x))/dx and then compute f'(1). write code  and check analytical gradient","metadata":{"execution":{"iopub.status.busy":"2025-01-10T14:03:25.011348Z","iopub.execute_input":"2025-01-10T14:03:25.011712Z","iopub.status.idle":"2025-01-10T14:03:25.016130Z","shell.execute_reply.started":"2025-01-10T14:03:25.011684Z","shell.execute_reply":"2025-01-10T14:03:25.014935Z"}}},{"cell_type":"code","source":"def f(x):\n    return (x-2)**2\ndef fp(x):\n    return 2*(x-2)\n\nx=torch.tensor([1.0],requires_grad=True)\n\n# pytorch\ny=f(x)\ny.backward()\n\nprint(\"Analytical derivative f'(x): \", fp(x))\nprint(\"Pytorch's derivative f'(x): \",x.grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:08:41.903887Z","iopub.execute_input":"2025-01-10T14:08:41.904244Z","iopub.status.idle":"2025-01-10T14:08:41.914926Z","shell.execute_reply.started":"2025-01-10T14:08:41.904218Z","shell.execute_reply":"2025-01-10T14:08:41.913630Z"}},"outputs":[{"name":"stdout","text":"Analytical derivative f'(x):  tensor([-2.], grad_fn=<MulBackward0>)\nPytorch's derivative f'(x):  tensor([-2.])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### Q3. define a function y=x^2+5. The function y will not only carry the result of evaluating x, but also gradient function dely/delx called grad_fn in the new tensor y. compare the result with analytical gradient","metadata":{"execution":{"iopub.status.busy":"2025-01-10T14:11:46.778609Z","iopub.execute_input":"2025-01-10T14:11:46.779141Z","iopub.status.idle":"2025-01-10T14:11:46.783963Z","shell.execute_reply.started":"2025-01-10T14:11:46.779106Z","shell.execute_reply":"2025-01-10T14:11:46.782741Z"}}},{"cell_type":"code","source":"import torch\n\nx=torch.tensor([2.0])\nx.requires_grad_(True)\n\ny=x**2+5\n\nprint(\"\\ny: \",y)\n\ny.backward() #dy/dx\nprint(\"\\nPyTorch Gradient: \",x.grad)\n\n# Analytical \ndy_dx=2*x\nprint(\"\\nAnalytical Gradient: \",dy_dx)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:17:21.980189Z","iopub.execute_input":"2025-01-10T14:17:21.980531Z","iopub.status.idle":"2025-01-10T14:17:21.992118Z","shell.execute_reply.started":"2025-01-10T14:17:21.980504Z","shell.execute_reply":"2025-01-10T14:17:21.990906Z"}},"outputs":[{"name":"stdout","text":"\ny:  tensor([9.], grad_fn=<AddBackward0>)\n\nPyTorch Gradient:  tensor([4.])\n\nAnalytical Gradient:  tensor([4.], grad_fn=<MulBackward0>)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"#### Q4 Write a fucntion to compute the gradient of the sigmoid function \n\n##### sigma(x)=s(c(b(a(x)))) where a(x)=-x\n##### b(a)=e^a\n##### c(b)=1+b\n##### s(c)=1/c\n","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef grad_sigmoid_manual(x):\n    a=-x\n    b=np.exp(a)\n    c=1+b\n    s=1.0/c\n\n    ds_dc=(-1.0/(c**2))\n    ds_db=ds_dc*1 # ds_dc* dc_db\n    ds_da=ds_db*np.exp(a) # ds_da=ds_dc*dc_db*db_da\n    ds_dx=ds_da*(-1) # ds_dx=ds_dc*dc_db*db_da*da_dx\n\n    return ds_dx\n\ndef sigmoid(x):\n    y=1.0 /(1.0+ torch.exp(-x))\n    return y\n\ninput_x=2.0\n\nx=torch.tensor(input_x).requires_grad_(True)\ny=sigmoid(x)\ny.backward()\n\n# comparing results of manual and automatic gradient functions\nprint(\"autograd: \",x.grad.item())\nprint(\"\\nmanual: \", grad_sigmoid_manual(input_x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T14:41:37.433055Z","iopub.execute_input":"2025-01-10T14:41:37.433421Z","iopub.status.idle":"2025-01-10T14:41:37.442679Z","shell.execute_reply.started":"2025-01-10T14:41:37.433392Z","shell.execute_reply":"2025-01-10T14:41:37.441448Z"}},"outputs":[{"name":"stdout","text":"autograd:  0.10499356687068939\n\nmanual:  0.1049935854035065\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Excercise","metadata":{"execution":{"iopub.status.busy":"2025-01-10T14:42:16.334461Z","iopub.execute_input":"2025-01-10T14:42:16.334971Z","iopub.status.idle":"2025-01-10T14:42:16.340112Z","shell.execute_reply.started":"2025-01-10T14:42:16.334919Z","shell.execute_reply":"2025-01-10T14:42:16.338865Z"}}},{"cell_type":"markdown","source":"#### Q1. draw computation graph and work out the gradient dz/da by following the path back from z to a and compare the result with the analytical gradient","metadata":{}},{"cell_type":"code","source":"import torch\n\ninput_a=2.0\ninput_b=3.0\n\na=torch.tensor(input_a, requires_grad=True)\nb=torch.tensor(input_b, requires_grad=True)\n\nx=2*a+3*b\ny=5*a*a + 3*b*b*b\nz=2*x+3*y\n\ndef manual(a,b):\n    x=2*a+3*b\n    y=5*a*a + 3*b**3\n    z=2*x+3*y\n                        #dz_dx=2 #dx_da=2\n    dz_da= 2*2 + 3*5*2*a  #dz_da=dz_dx*dx_da+ dz_dy*dy_da\n\n    dz_db=2*3*1 + 3*3*3*b**2\n\n    return dz_da,dz_db\n\n\nprint(\"x: \",x)\nprint(\"y: \",y)\nprint(\"z: \",z)\n\nprint(f\"\\nManual Analytical Derivative at a={input_a} and b={input_b} :{manual(input_a,input_b)}\")\n\nz.backward()\nprint(f\"\\nPytorch's derivatives at a={a} and b={b}: \")\nprint(\"dz_da: \",a.grad)\nprint(\"dz_db: \",b.grad)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:13:55.843283Z","iopub.execute_input":"2025-01-10T15:13:55.843669Z","iopub.status.idle":"2025-01-10T15:13:55.859092Z","shell.execute_reply.started":"2025-01-10T15:13:55.843622Z","shell.execute_reply":"2025-01-10T15:13:55.858050Z"}},"outputs":[{"name":"stdout","text":"x:  tensor(13., grad_fn=<AddBackward0>)\ny:  tensor(101., grad_fn=<AddBackward0>)\nz:  tensor(329., grad_fn=<AddBackward0>)\n\nManual Analytical Derivative at a=2.0 and b=3.0 :(64.0, 249.0)\n\nPytorch's derivatives at a=2.0 and b=3.0: \ndz_da:  tensor(64.)\ndz_db:  tensor(249.)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"#### Q2. same question\n","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:14:34.138440Z","iopub.execute_input":"2025-01-10T15:14:34.138812Z","iopub.status.idle":"2025-01-10T15:14:34.143446Z","shell.execute_reply.started":"2025-01-10T15:14:34.138778Z","shell.execute_reply":"2025-01-10T15:14:34.141971Z"}}},{"cell_type":"code","source":"def manual(b,x,w):\n    u=w*x\n    v=u+b\n    a=max(0.0, v)\n\n    if a==0:\n        return 0\n    else:\n        da_dv=1\n        da_du=1   #da_dv*dv_du\n        da_db=1   #da_dv*dv_db\n        da_dw=x   #da_dv*dv_du*du_dw\n        da_dx=w   #da_dv*dv_du*du_dx\n\n    return da_dw,da_db,da_dx\n\ninp_x = 3.0\ninp_w = 4.0\ninp_b = 5.0\n\nx = torch.tensor(inp_x,requires_grad=True)\nw = torch.tensor(inp_w,requires_grad = True)\nb = torch.tensor(inp_b,requires_grad = True)\n\nu = w*x;u.retain_grad()\nv = u + b;v.retain_grad()\nrelu = torch.nn.ReLU();\na = relu(v);a.retain_grad()\n\na.backward()\nprint(\"PyTorch Derivatives: \")\nprint(\"da_dw: \",w.grad)\nprint(\"da_db: \",b.grad)\nprint(\"da_dx: \",x.grad)\n\ngrad_w,grad_b,grad_x= manual(inp_b,inp_x,inp_w)\nprint(\"\\nManual da_dw: \",grad_w)\nprint(\"Manual da_db: \",grad_b)\nprint(\"Manual da_dx: \",grad_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:38:37.725059Z","iopub.execute_input":"2025-01-10T15:38:37.725404Z","iopub.status.idle":"2025-01-10T15:38:37.742774Z","shell.execute_reply.started":"2025-01-10T15:38:37.725379Z","shell.execute_reply":"2025-01-10T15:38:37.741630Z"}},"outputs":[{"name":"stdout","text":"PyTorch Derivatives: \nda_dw:  tensor(3.)\nda_db:  tensor(1.)\nda_dx:  tensor(4.)\n\nManual da_dw:  3.0\nManual da_db:  1\nManual da_dx:  4.0\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"#### Q3. Same Problem sigmoid Function","metadata":{}},{"cell_type":"markdown","source":"#### Q4. analytical gradient and analytical gradient of function =e^(-x^2-2*x-sin(x)) wrt x","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:25:49.477628Z","iopub.execute_input":"2025-01-10T15:25:49.478038Z","iopub.status.idle":"2025-01-10T15:25:49.482381Z","shell.execute_reply.started":"2025-01-10T15:25:49.478006Z","shell.execute_reply":"2025-01-10T15:25:49.480991Z"}}},{"cell_type":"code","source":"def manual(x):\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Q5. same question. function is 8*x**4+ 3*x**3 + 7*x**2 +6*x +3","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:27:17.212286Z","iopub.execute_input":"2025-01-10T15:27:17.212622Z","iopub.status.idle":"2025-01-10T15:27:17.216734Z","shell.execute_reply.started":"2025-01-10T15:27:17.212598Z","shell.execute_reply":"2025-01-10T15:27:17.215573Z"}}},{"cell_type":"code","source":"def manual(x):\n    y=8*x**4+ 3*x**3 + 7*x**2 +6*x +3\n    dy_dx=32*(x**3) + 9*(x**2) + 14*x + 6\n    return dy_dx\n    \ninp_x=3.0\nx = torch.tensor(inp_x,requires_grad=True)\na=8*x**4+ 3*x**3 + 7*x**2 +6*x +3\n\na.backward()\n\nprint(\"PyTorch Derivatives: \")\nprint(\"da_dx: \",x.grad)\n\nprint(\"Manual da_dx: \",manual(inp_x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:47:24.647563Z","iopub.execute_input":"2025-01-10T15:47:24.647955Z","iopub.status.idle":"2025-01-10T15:47:24.657619Z","shell.execute_reply.started":"2025-01-10T15:47:24.647925Z","shell.execute_reply":"2025-01-10T15:47:24.656590Z"}},"outputs":[{"name":"stdout","text":"PyTorch Derivatives: \nda_dx:  tensor(993.)\nManual da_dx:  993.0\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"#### Q6. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}